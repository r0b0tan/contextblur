# Base URL for the Ollama inference server.
# Used by src/llm/ollama.ts. Only required when llm.enabled=true in requests.
OLLAMA_BASE_URL=http://localhost:11434

# Server bind settings (optional)
PORT=3000
HOST=0.0.0.0
